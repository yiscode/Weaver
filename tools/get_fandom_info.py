import requests
import json
import hashlib
import time
from urllib.parse import unquote

def get_page_info_and_content(title):
    base_url = "https://typemoon.fandom.com/api.php"
    # ä½¿ç”¨ query + export æ¨¡å¼ï¼Œé€™ç¨®æ¨¡å¼å°ç‰¹æ®Šå­—ç¬¦æ¨™é¡Œæœ€å‹å–„
    params = {
        "action": "query",
        "format": "json",
        "titles": title,
        "prop": "revisions|categories",
        "rvprop": "content", # æŠ“å–åŸå§‹ç¢¼
        "redirects": 1,
        "cllimit": "max"
    }

    try:

        res = requests.get(base_url, params=params).json()
        # è™•ç†é‡æ–°å°å‘çš„é‚è¼¯
        if "query" in res and "redirects" in res["query"]:
            real_title = res["query"]["redirects"][0]["to"]
            print(f"(Redirect -> {real_title})", end=" ")

        pages = res.get("query", {}).get("pages", {})
        page_id = next(iter(pages))


        if page_id == "-1":
            return None, None

        page_data = pages[page_id]

        # å–å¾—åŸå§‹ WikiText
        raw_text = page_data.get("revisions", [{}])[0].get("*", "")

        # ç°¡å–®æ¸…æ´—ï¼šå»æ‰ Wiki çš„ [[ ]] å’Œ {{ }} æ¨™ç±¤
        import re
# --- æ”¹é€²æ¸…æ´—é‚è¼¯ ---
        # ä¸è¦ç›´æ¥åˆªé™¤æ¨£æ¿ï¼Œæ”¹ç‚ºæå–è£¡é¢çš„æ–‡å­—ï¼Œæˆ–è€…åªåˆªé™¤ç‰¹å®šçš„ç³»çµ±æ¨™ç±¤
        clean_text = raw_text
        # åªå»æ‰ Wiki é€£çµç¬¦è™Ÿï¼Œä¿ç•™è£¡é¢çš„æ–‡å­—
        clean_text = re.sub(r'\[\[(?:[^|\]]*\|)?([^\]]+)\]\]', r'\1', clean_text)
        # å»æ‰ ''' (ç²—é«”)
        clean_text = clean_text.replace("'''", "").replace("''", "")
        # å»æ‰ <ref> æ¨™ç±¤
        clean_text = re.sub(r'<ref.*?>.*?</ref>', '', clean_text, flags=re.DOTALL)
        # å»æ‰ HTML è¨»é‡‹
        clean_text = re.sub(r'', '', clean_text, flags=re.DOTALL)

        # åˆ¤å®šé¡å‹
        categories = [c["title"] for c in page_data.get("categories", [])]
        ltype = "lore"
        # åªè¦åˆ†é¡è£¡æœ‰ Abnormalityï¼Œä¸è«–å¤§å°å¯«
        cat_str = "|".join(categories).lower()
        if "abnormality" in cat_str or "abnormalities" in cat_str:
            ltype = "abnormality"
        elif "character" in cat_str or "sephirah" in cat_str:
            ltype = "character"

        return clean_text.strip(), ltype
    except Exception as e:
        return None, None

def process_urls_to_jsonl(input_file="wiki_urls.txt", output_file="pm_lore_final.jsonl"):
    count = 0
    with open(input_file, "r", encoding="utf-8") as f, \
         open(output_file, "w", encoding="utf-8") as out:

        for line in f:
            url = line.strip()
            if not url or "/wiki/" not in url: continue

            raw_title = url.split("/wiki/")[-1]
            title = unquote(raw_title).replace("_", " ")

            # Debug: é¡¯ç¤ºæ­£åœ¨å˜—è©¦çš„æ¨™é¡Œ
            print(f"ğŸ” å˜—è©¦æŠ“å–: [{title}]", end=" ")

            content, ltype = get_page_info_and_content(title)

            # æ”¾å¯¬é™åˆ¶åˆ° 20 å€‹å­—ï¼Œå› ç‚ºæœ‰äº›ç·¨è™Ÿé é¢çœŸçš„å¾ˆçŸ­
            if content and len(content) > 20:
                entry = {
                    "id": f"pm_{hashlib.md5(title.encode()).hexdigest()[:8]}",
                    "type": ltype,
                    "name": title,
                    "text_zh": content,
                    "source_url": url,
                    "content_hash": hashlib.sha256(content.encode()).hexdigest(),
                    "updated_at": time.strftime("%Y-%m-%dT%H:%M:%SZ")
                }
                out.write(json.dumps(entry, ensure_ascii=False) + "\n")
                count += 1
                print(f"-> âœ… æˆåŠŸ ({ltype})")
            else:
                print("-> âŒ å…§å®¹ä¸è¶³")

            time.sleep(0.2)

    print(f"\nâœ¨ ä»»å‹™å®Œæˆï¼å…±å­˜å…¥ {count} ç­†ã€‚")

if __name__ == "__main__":
    process_urls_to_jsonl()